
![](image/Pasted%20image%2020250311135407.png)

# 锁的基础是什么

是建立在两大基础之上，原子操作 + 信号锁

## atomic 操作

下面的代码，估计是 1000 ，但是实际结果并不是，这就是并发问题，原因是 `*p++` 相当于三步，读取，修改，写入

![](image/Pasted%20image%2020250311143442.png)

```go
package main  
  
import (  
    "fmt"  
    "time")  
  
func add(i *int32) {  
    *i++  
}  
  
func main() {  
    res := int32(0)  
    for i := 0; i < 1000; i++ {  
       go add(&res)  
    }  
    time.Sleep(time.Second)  
    fmt.Println(res)  
}
```

但是用如下的代码就没问题，去看看具体实现

![](image/Pasted%20image%2020250311145143.png)

```go
package main  
  
import (  
    "fmt"  
    "sync/atomic"    
    "time"
)  
  
func add(i *int32) {  
    //*i++  
    atomic.AddInt32(i, 1)  
}  
  
func main() {  
    res := int32(0)  
    for i := 0; i < 1000; i++ {  
       go add(&res)  
    }  
    time.Sleep(time.Second)  
    fmt.Println(res)  
}

// AddInt32 atomically adds delta to *addr and returns the new value.
// Consider using the more ergonomic and less error-prone [Int32.Add] instead.
//
//go:noescape
func AddInt32(addr *int32, delta int32) (new int32)

TEXT ·AddInt32(SB),NOSPLIT,$0  
    JMP    internal∕runtime∕atomic·Xadd(SB)

// uint32 Xadd(uint32 volatile *val, int32 delta)
// Atomically:
//	*val += delta;
//	return *val;
TEXT ·Xadd(SB), NOSPLIT, $0-20
	MOVQ	ptr+0(FP), BX
	MOVL	delta+8(FP), AX
	MOVL	AX, CX
	LOCK
	XADDL	AX, 0(BX)
	ADDL	CX, AX
	MOVL	AX, ret+16(FP)
	RET
```

## sema 锁

![](image/Pasted%20image%2020250311165021.png)

![](image/Pasted%20image%2020250311165500.png)

```go
// A semaRoot holds a balanced tree of sudog with distinct addresses (s.elem).
// Each of those sudog may in turn point (through s.waitlink) to a list
// of other sudogs waiting on the same address.
// The operations on the inner lists of sudogs with the same address
// are all O(1). The scanning of the top-level semaRoot list is O(log n),
// where n is the number of distinct addresses with goroutines blocked
// on them that hash to the given semaRoot.
// See golang.org/issue/17953 for a program that worked badly
// before we introduced the second level of list, and
// BenchmarkSemTable/OneAddrCollision/* for a benchmark that exercises this.
type semaRoot struct {
	lock  mutex
	treap *sudog        // root of balanced tree of unique waiters.
	nwait atomic.Uint32 // Number of waiters. Read w/o the lock.
}


// A Mutex is a mutual exclusion lock.
//
// See package [sync.Mutex] documentation.
type Mutex struct {
	state int32
	sema  uint32
}

// sudog (pseudo-g) represents a g in a wait list, such as for sending/receiving
// on a channel.
//
// sudog is necessary because the g ↔ synchronization object relation
// is many-to-many. A g can be on many wait lists, so there may be
// many sudogs for one g; and many gs may be waiting on the same
// synchronization object, so there may be many sudogs for one object.
//
// sudogs are allocated from a special pool. Use acquireSudog and
// releaseSudog to allocate and free them.
type sudog struct {
	// The following fields are protected by the hchan.lock of the
	// channel this sudog is blocking on. shrinkstack depends on
	// this for sudogs involved in channel ops.

	g *g

	next *sudog
	prev *sudog
	elem unsafe.Pointer // data element (may point to stack)

	// The following fields are never accessed concurrently.
	// For channels, waitlink is only accessed by g.
	// For semaphores, all fields (including the ones above)
	// are only accessed when holding a semaRoot lock.

	acquiretime int64
	releasetime int64
	ticket      uint32

	// isSelect indicates g is participating in a select, so
	// g.selectDone must be CAS'd to win the wake-up race.
	isSelect bool

	// success indicates whether communication over channel c
	// succeeded. It is true if the goroutine was awoken because a
	// value was delivered over channel c, and false if awoken
	// because c was closed.
	success bool

	// waiters is a count of semaRoot waiting list other than head of list,
	// clamped to a uint16 to fit in unused space.
	// Only meaningful at the head of the list.
	// (If we wanted to be overly clever, we could store a high 16 bits
	// in the second entry in the list.)
	waiters uint16

	parent   *sudog // semaRoot binary tree
	waitlink *sudog // g.waiting list or semaRoot
	waittail *sudog // semaRoot
	c        *hchan // channel
}

```

### sema 操作(uint32 > 0)

![](image/Pasted%20image%2020250311170423.png)

具体逻辑请参考[相关代码](#代码)

![](image/Pasted%20image%2020250311170531.png)

### sema 操作(uint 32 == 0)

![](image/Pasted%20image%2020250311171114.png)

### 代码

```go
// Called from runtime.  
func semacquire(addr *uint32) {  
    semacquire1(addr, false, 0, 0, waitReasonSemacquire)  
}  
  
func semacquire1(addr *uint32, lifo bool, profile semaProfileFlags, skipframes int, reason waitReason) {  
    gp := getg()  
    if gp != gp.m.curg {  
       throw("semacquire not on the G stack")  
    }  
  
    // Easy case.  
    if cansemacquire(addr) {  
       return  
    }  
  
    // Harder case:  
    // increment waiter count    // try cansemacquire one more time, return if succeeded    // enqueue itself as a waiter    // sleep    // (waiter descriptor is dequeued by signaler)    s := acquireSudog()  
    root := semtable.rootFor(addr)  
    t0 := int64(0)  
    s.releasetime = 0  
    s.acquiretime = 0  
    s.ticket = 0  
    if profile&semaBlockProfile != 0 && blockprofilerate > 0 {  
       t0 = cputicks()  
       s.releasetime = -1  
    }  
    if profile&semaMutexProfile != 0 && mutexprofilerate > 0 {  
       if t0 == 0 {  
          t0 = cputicks()  
       }  
       s.acquiretime = t0  
    }  
    for {  
       lockWithRank(&root.lock, lockRankRoot)  
       // Add ourselves to nwait to disable "easy case" in semrelease.  
       root.nwait.Add(1)  
       // Check cansemacquire to avoid missed wakeup.  
       if cansemacquire(addr) {  
          root.nwait.Add(-1)  
          unlock(&root.lock)  
          break  
       }  
       // Any semrelease after the cansemacquire knows we're waiting  
       // (we set nwait above), so go to sleep.       root.queue(addr, s, lifo)  
       goparkunlock(&root.lock, reason, traceBlockSync, 4+skipframes)  
       if s.ticket != 0 || cansemacquire(addr) {  
          break  
       }  
    }  
    if s.releasetime > 0 {  
       blockevent(s.releasetime-t0, 3+skipframes)  
    }  
    releaseSudog(s)  
}


func cansemacquire(addr *uint32) bool {
	for {
		v := atomic.Load(addr)
		if v == 0 {
			return false
		}
		if atomic.Cas(addr, v, v-1) {
			return true
		}
	}
}

func semrelease(addr *uint32) {
	semrelease1(addr, false, 0)
}

func semrelease1(addr *uint32, handoff bool, skipframes int) {
	root := semtable.rootFor(addr)
	atomic.Xadd(addr, 1)

	// Easy case: no waiters?
	// This check must happen after the xadd, to avoid a missed wakeup
	// (see loop in semacquire).
	if root.nwait.Load() == 0 {
		return
	}

	// Harder case: search for a waiter and wake it.
	lockWithRank(&root.lock, lockRankRoot)
	if root.nwait.Load() == 0 {
		// The count is already consumed by another goroutine,
		// so no need to wake up another goroutine.
		unlock(&root.lock)
		return
	}
	s, t0, tailtime := root.dequeue(addr)
	if s != nil {
		root.nwait.Add(-1)
	}
	unlock(&root.lock)
	if s != nil { // May be slow or even yield, so unlock first
		acquiretime := s.acquiretime
		if acquiretime != 0 {
			// Charge contention that this (delayed) unlock caused.
			// If there are N more goroutines waiting beyond the
			// one that's waking up, charge their delay as well, so that
			// contention holding up many goroutines shows up as
			// more costly than contention holding up a single goroutine.
			// It would take O(N) time to calculate how long each goroutine
			// has been waiting, so instead we charge avg(head-wait, tail-wait)*N.
			// head-wait is the longest wait and tail-wait is the shortest.
			// (When we do a lifo insertion, we preserve this property by
			// copying the old head's acquiretime into the inserted new head.
			// In that case the overall average may be slightly high, but that's fine:
			// the average of the ends is only an approximation to the actual
			// average anyway.)
			// The root.dequeue above changed the head and tail acquiretime
			// to the current time, so the next unlock will not re-count this contention.
			dt0 := t0 - acquiretime
			dt := dt0
			if s.waiters != 0 {
				dtail := t0 - tailtime
				dt += (dtail + dt0) / 2 * int64(s.waiters)
			}
			mutexevent(dt, 3+skipframes)
		}
		if s.ticket != 0 {
			throw("corrupted semaphore ticket")
		}
		if handoff && cansemacquire(addr) {
			s.ticket = 1
		}
		readyWithTime(s, 5+skipframes)
		if s.ticket == 1 && getg().m.locks == 0 {
			// Direct G handoff
			// readyWithTime has added the waiter G as runnext in the
			// current P; we now call the scheduler so that we start running
			// the waiter G immediately.
			// Note that waiter inherits our time slice: this is desirable
			// to avoid having a highly contended semaphore hog the P
			// indefinitely. goyield is like Gosched, but it emits a
			// "preempted" trace event instead and, more importantly, puts
			// the current G on the local runq instead of the global one.
			// We only do this in the starving regime (handoff=true), as in
			// the non-starving case it is possible for a different waiter
			// to acquire the semaphore while we are yielding/scheduling,
			// and this would be wasteful. We wait instead to enter starving
			// regime, and then we start to do direct handoffs of ticket and
			// P.
			// See issue 33747 for discussion.
			goyield()
		}
	}
}

```

## 总结

![](image/Pasted%20image%2020250311171229.png)

# 互斥锁

![](image/Pasted%20image%2020250311171656.png)

在结构体中添加锁

```go
package main

import (
	"fmt"
	"sync"
	"time"
)

type Person struct {
	mu     sync.Mutex
	salary int
	level  int
}

func promote(p *Person) {
	p.mu.Lock()
	p.salary++
	fmt.Println(p.salary)
	p.level++
	fmt.Println(p.level)
	p.mu.Unlock()
}

func main() {

	p := Person{level: 1, salary: 10000}

	go promote(&p)
	go promote(&p)
	go promote(&p)

	time.Sleep(time.Second)

}
```

还有一种方式是，但是这种方案就像自旋锁，线程会空转，没有触发基于信号的调度就会一直运行，竞争激烈不可取

```go
package main

import (
	"fmt"
	"sync/atomic"
	"time"
)

type Person struct {
	mu     atomic.Int32
	salary int
	level  int
}

func promote(p *Person) {
	p.mu.CompareAndSwap(0, 1)
	p.salary++
	fmt.Println(p.salary)
	p.level++
	fmt.Println(p.level)
	p.mu.CompareAndSwap(1, 0)
}

func main() {

	p := Person{level: 1, salary: 10000}

	go promote(&p)
	go promote(&p)
	go promote(&p)

	time.Sleep(time.Second)

}
```

## sync.mutex

state 低三位都代表不同含义，剩下高 29 位表示等待的数量，由于 sema 初始化为 0 ，可以看作阻塞队列，state 表示多个含义，低 3 位 表示饥饿、醒来、锁住，高 29 位是排队数量

![](image/Pasted%20image%2020250311205201.png)

### 正常模式加锁

![](image/Pasted%20image%2020250311205527.png)

解释下下面两张图，多个携程通过获取位来获取锁，褐色就是获取到锁，带箭头就是自旋尝试，一定次数没抢到就到 SeamRoot 去休眠，相当于阻塞队列，有协程忙完了就唤醒阻塞的协程

![](image/Pasted%20image%2020250311205934.png)

![](image/Pasted%20image%2020250311205947.png)

代码中，上锁的时候通过 CSA 上锁，抢到锁就没事，没抢到就开始自旋，满足某些条件就进入阻塞队列

### 正常模式解锁

![](image/Pasted%20image%2020250315222322.png)

首先就是释放锁，看 waitershift 是否不为 0 ，如果不为 0 就使用 sema release ，放出来一个进入调度

![](image/Pasted%20image%2020250315222345.png)

![](image/Pasted%20image%2020250315222420.png)

但是有个问题，就是放出来的时候可能依然有多个协程抢锁，这是一个问题，详情可以看代码

![](image/Pasted%20image%2020250315222809.png)

### 正常模式总结

![](image/Pasted%20image%2020250315225916.png)

### 锁饥饿问题

就是在正常模式下，解锁了还是要和新来的协程来竞争锁，运气不好可能会导致一直处于饥饿状态所以这个 state 标识位就设置了 starving 标志位来表示饥饿

![](image/Pasted%20image%2020250316172725.png)

### 饥饿模式

![](image/Pasted%20image%2020250316200948.png)

只要有任何一个协程等待时间超过 1 ms，state 其中的 starving 位就被置为 1 ，这个时候只要没抢到就直接休眠，不会自旋，因此某个协程释放锁后，基本上不会有协程竞争锁，唤醒的协程会直接获取到这把锁，新来的会直接进入到 sema 队列，不会去抢夺，所有的协程都完成了任务就回到了正常的模式，这个意义就是在有很大概率冲突的情况下就不要自旋了，抢不到就直接休眠，减少了系统的消耗。

### 代码

```go
// A Mutex is a mutual exclusion lock.
//
// See package [sync.Mutex] documentation.
type Mutex struct {
	state int32
	sema  uint32
}

const (
	mutexLocked = 1 << iota // mutex is locked
	mutexWoken
	mutexStarving
	mutexWaiterShift = iota

	// Mutex fairness.
	//
	// Mutex can be in 2 modes of operations: normal and starvation.
	// In normal mode waiters are queued in FIFO order, but a woken up waiter
	// does not own the mutex and competes with new arriving goroutines over
	// the ownership. New arriving goroutines have an advantage -- they are
	// already running on CPU and there can be lots of them, so a woken up
	// waiter has good chances of losing. In such case it is queued at front
	// of the wait queue. If a waiter fails to acquire the mutex for more than 1ms,
	// it switches mutex to the starvation mode.
	//
	// In starvation mode ownership of the mutex is directly handed off from
	// the unlocking goroutine to the waiter at the front of the queue.
	// New arriving goroutines don't try to acquire the mutex even if it appears
	// to be unlocked, and don't try to spin. Instead they queue themselves at
	// the tail of the wait queue.
	//
	// If a waiter receives ownership of the mutex and sees that either
	// (1) it is the last waiter in the queue, or (2) it waited for less than 1 ms,
	// it switches mutex back to normal operation mode.
	//
	// Normal mode has considerably better performance as a goroutine can acquire
	// a mutex several times in a row even if there are blocked waiters.
	// Starvation mode is important to prevent pathological cases of tail latency.
	starvationThresholdNs = 1e6
)

// Lock locks m.
//
// See package [sync.Mutex] documentation.
func (m *Mutex) Lock() {
	// Fast path: grab unlocked mutex.
	if atomic.CompareAndSwapInt32(&m.state, 0, mutexLocked) {
		if race.Enabled {
			race.Acquire(unsafe.Pointer(m))
		}
		return
	}
	// Slow path (outlined so that the fast path can be inlined)
	m.lockSlow()
}

// TryLock tries to lock m and reports whether it succeeded.
//
// See package [sync.Mutex] documentation.
func (m *Mutex) TryLock() bool {
	old := m.state
	if old&(mutexLocked|mutexStarving) != 0 {
		return false
	}

	// There may be a goroutine waiting for the mutex, but we are
	// running now and can try to grab the mutex before that
	// goroutine wakes up.
	if !atomic.CompareAndSwapInt32(&m.state, old, old|mutexLocked) {
		return false
	}

	if race.Enabled {
		race.Acquire(unsafe.Pointer(m))
	}
	return true
}

func (m *Mutex) lockSlow() {
	var waitStartTime int64
	starving := false
	awoke := false
	iter := 0
	old := m.state
	for {
		// Don't spin in starvation mode, ownership is handed off to waiters
		// so we won't be able to acquire the mutex anyway.
		// 被锁且没有饥饿
		if old&(mutexLocked|mutexStarving) == mutexLocked && runtime_canSpin(iter) {
			// Active spinning makes sense.
			// Try to set mutexWoken flag to inform Unlock
			// to not wake other blocked goroutines.
			if !awoke && old&mutexWoken == 0 && old>>mutexWaiterShift != 0 &&
				atomic.CompareAndSwapInt32(&m.state, old, old|mutexWoken) {
				awoke = true
			}
			runtime_doSpin()
			iter++
			old = m.state
			continue
		}
		new := old
		// Don't try to acquire starving mutex, new arriving goroutines must queue.
		// 没有处于饥饿状态，直接标记上
		if old&mutexStarving == 0 {
			new |= mutexLocked
		}
		//  被锁或者饥饿，等待数量加一
		if old&(mutexLocked|mutexStarving) != 0 {
			new += 1 << mutexWaiterShift
		}
		// The current goroutine switches mutex to starvation mode.
		// But if the mutex is currently unlocked, don't do the switch.
		// Unlock expects that starving mutex has waiters, which will not
		// be true in this case.
		// 如果确实是饥饿，那么就会写入标志位，判断在下面，常量定义在上面，这里只是设置好了值，没有写入
		if starving && old&mutexLocked != 0 {
			new |= mutexStarving
		}
		if awoke {
			// The goroutine has been woken from sleep,
			// so we need to reset the flag in either case.
			if new&mutexWoken == 0 {
				throw("sync: inconsistent mutex state")
			}
			new &^= mutexWoken
		}
		// 这里就是写入的逻辑，设置后就是处于饥饿模式
		if atomic.CompareAndSwapInt32(&m.state, old, new) {
			if old&(mutexLocked|mutexStarving) == 0 {
				break // locked the mutex with CAS
			}
			// If we were already waiting before, queue at the front of the queue.
			queueLifo := waitStartTime != 0
			if waitStartTime == 0 {
				waitStartTime = runtime_nanotime()
			}
			// 回到上文中获取 sema 锁的操作，由于初始化为 0，只要申请就会阻塞，恢复也是从这里开始
			runtime_SemacquireMutex(&m.sema, queueLifo, 2)
			// 这里就是计算是否饥饿的逻辑，这个是 1e6 nano = 1 ms，然后回头重新执行
			starving = starving || runtime_nanotime()-waitStartTime > starvationThresholdNs
			old = m.state
			// 状态是饥饿模式，条件就是处于饥饿状态被唤醒，直接加上锁，然后循环上去沉睡
			if old&mutexStarving != 0 {
				// If this goroutine was woken and mutex is in starvation mode,
				// ownership was handed off to us but mutex is in somewhat
				// inconsistent state: mutexLocked is not set and we are still
				// accounted as waiter. Fix that.
				if old&(mutexLocked|mutexWoken) != 0 || old>>mutexWaiterShift == 0 {
					throw("sync: inconsistent mutex state")
				}
				delta := int32(mutexLocked - 1<<mutexWaiterShift)
				if !starving || old>>mutexWaiterShift == 1 {
					// Exit starvation mode.
					// Critical to do it here and consider wait time.
					// Starvation mode is so inefficient, that two goroutines
					// can go lock-step infinitely once they switch mutex
					// to starvation mode.
					delta -= mutexStarving
				}
				atomic.AddInt32(&m.state, delta)
				break
			}
			awoke = true
			iter = 0
		} else {
			old = m.state
		}
	}

	if race.Enabled {
		race.Acquire(unsafe.Pointer(m))
	}
}

// Unlock unlocks m.
//
// See package [sync.Mutex] documentation.
func (m *Mutex) Unlock() {
	if race.Enabled {
		_ = m.state
		race.Release(unsafe.Pointer(m))
	}

	// Fast path: drop lock bit.
	new := atomic.AddInt32(&m.state, -mutexLocked)
	if new != 0 {
		// Outlined slow path to allow inlining the fast path.
		// To hide unlockSlow during tracing we skip one extra frame when tracing GoUnblock.
		m.unlockSlow(new)
	}
}

func (m *Mutex) unlockSlow(new int32) {
	if (new+mutexLocked)&mutexLocked == 0 {
		fatal("sync: unlock of unlocked mutex")
	}
	if new&mutexStarving == 0 {
		old := new
		for {
			// If there are no waiters or a goroutine has already
			// been woken or grabbed the lock, no need to wake anyone.
			// In starvation mode ownership is directly handed off from unlocking
			// goroutine to the next waiter. We are not part of this chain,
			// since we did not observe mutexStarving when we unlocked the mutex above.
			// So get off the way.
			if old>>mutexWaiterShift == 0 || old&(mutexLocked|mutexWoken|mutexStarving) != 0 {
				return
			}
			// Grab the right to wake someone.
			new = (old - 1<<mutexWaiterShift) | mutexWoken
			if atomic.CompareAndSwapInt32(&m.state, old, new) {
				runtime_Semrelease(&m.sema, false, 2)
				return
			}
			old = m.state
		}
	} else {
		// Starving mode: handoff mutex ownership to the next waiter, and yield
		// our time slice so that the next waiter can start to run immediately.
		// Note: mutexLocked is not set, the waiter will set it after wakeup.
		// But mutex is still considered locked if mutexStarving is set,
		// so new coming goroutines won't acquire it.
		runtime_Semrelease(&m.sema, true, 2)
	}
}
```

## 总结

![](image/Pasted%20image%2020250316205638.png)

## 使用经验

![](image/Pasted%20image%2020250316210132.png)

# 读写锁

很多业务需要读取最新的数据，不希望还在读取时数据就被篡改，而且此时使用传统的互斥锁效率太低了，需要并发读的时候，只能同时由一个协程进行，这个时候就需要使用读写锁，我只需要保证没有人改，使用 sync.RWMutex

```go
package main

import (
	"fmt"
	"sync"
	"time"
)

type Person struct {
	mu     sync.RWMutex
	salary int
	level  int
}

func promote(p *Person) {
	p.mu.Lock()
	p.salary++
	fmt.Println(p.salary)
	p.level++
	fmt.Println(p.level)
	p.mu.Unlock()
}

func printPerson(p *Person) {
	defer p.mu.RUnlock()
	p.mu.RLock()
	fmt.Println(p.salary)
	fmt.Println(p.level)
}

func main() {
	p := Person{level: 1, salary: 10000}
	go printPerson(&p)
	go printPerson(&p)
	go printPerson(&p)
	go promote(&p)
	time.Sleep(time.Second)
	time.Sleep(time.Second)
}
```

## 多个协程同时只读

![](image/Pasted%20image%2020250316213027.png)

## 读写锁介绍

本质上就是两把锁，一把独占写锁，一把共享读锁，都有自己的等待队列

![](image/Pasted%20image%2020250316213302.png)

## 读写锁需求

![](image/Pasted%20image%2020250316213502.png)

## 加写锁

### 没有读协程

这里就是竞争互斥锁，上文已经详细的讲到了，竞争 state 的最后一位，此时 readerCount = 0，如图所示，此时协程就需要竞争 w 这个锁，才有资格加上写锁，加写锁的操作是操作 readerCount ，如下图第二张图所示，减去一个常量变为负数，此时写锁就已经成功加上了，就可以干自己的业务员

![](image/Pasted%20image%2020250317123317.png)

![](image/Pasted%20image%2020250317123513.png)

### 有读协程

此时有 3 个读协程，加了读锁，写协程还是需要竞争互斥锁，此时只是将 3 - remutexMaxReader，将 readerCount 变为负数，这个负数表示后面的协程不要加读锁了，有协程需要写入，并且还保留了 3 这个信息，表示前面有 3 个读锁，需要等待 3 个读锁，此时的 readerWait = 3 ，这个变量记录了什么时候才能释放下一个写协程，也就是 3 个读锁释放后才能才能释放写协程，并且将读协程放入 writerSem 

![](image/Pasted%20image%2020250317124932.png)

![](image/Pasted%20image%2020250317125423.png)

![](image/Pasted%20image%2020250317125633.png)

### 总结

![](image/Pasted%20image%2020250317130515.png)

## 解写锁

![](image/Pasted%20image%2020250317140420.png)

![](image/Pasted%20image%2020250317140338.png)

![](image/Pasted%20image%2020250317140359.png)

## 加读锁

### readerCount > 0

抢到 w，就直接 readerCount + 1

![](image/Pasted%20image%2020250317141913.png)

### readerCount < 0

这个就有点复杂，说明此时有写协程，读协程都阻塞在 readerSem，此时将 readerCount + 1，自己去读队列里面排队，当写锁释放时，读协程就会执行

![](image/Pasted%20image%2020250317142020.png)
### 加读锁总结

![](image/Pasted%20image%2020250317142522.png)

## 解读锁

### readerCount > 0

直接 readerCount -1，就不用管了

![](image/Pasted%20image%2020250317143852.png)

### readerCount < 0

表示有写协程在等待，将 readerWait 和 readerCount 都 -1，当 readerWait 减到 0 时，代表可以释放读协程

![](image/Pasted%20image%2020250317144248.png)

![](image/Pasted%20image%2020250317144410.png)

### 总结

![](image/Pasted%20image%2020250317144554.png)

## 代码实现

这个就是 RWMutex 结构体，下面的图片就是介绍里面变量的含义

![](image/Pasted%20image%2020250316221922.png)

![](image/Pasted%20image%2020250316224621.png)

```go
// There is a modified copy of this file in runtime/rwmutex.go.
// If you make any changes here, see if you should make them there.

// A RWMutex is a reader/writer mutual exclusion lock.
// The lock can be held by an arbitrary number of readers or a single writer.
// The zero value for a RWMutex is an unlocked mutex.
//
// A RWMutex must not be copied after first use.
//
// If any goroutine calls [RWMutex.Lock] while the lock is already held by
// one or more readers, concurrent calls to [RWMutex.RLock] will block until
// the writer has acquired (and released) the lock, to ensure that
// the lock eventually becomes available to the writer.
// Note that this prohibits recursive read-locking.
// A [RWMutex.RLock] cannot be upgraded into a [RWMutex.Lock],
// nor can a [RWMutex.Lock] be downgraded into a [RWMutex.RLock].
//
// In the terminology of [the Go memory model],
// the n'th call to [RWMutex.Unlock] “synchronizes before” the m'th call to Lock
// for any n < m, just as for [Mutex].
// For any call to RLock, there exists an n such that
// the n'th call to Unlock “synchronizes before” that call to RLock,
// and the corresponding call to [RWMutex.RUnlock] “synchronizes before”
// the n+1'th call to Lock.
//
// [the Go memory model]: https://go.dev/ref/mem
type RWMutex struct {
	w           Mutex        // held if there are pending writers
	writerSem   uint32       // semaphore for writers to wait for completing readers
	readerSem   uint32       // semaphore for readers to wait for completing writers
	readerCount atomic.Int32 // number of pending readers
	readerWait  atomic.Int32 // number of departing readers
}

const rwmutexMaxReaders = 1 << 30


// Lock locks rw for writing.
// If the lock is already locked for reading or writing,
// Lock blocks until the lock is available.
func (rw *RWMutex) Lock() {
	if race.Enabled {
		race.Read(unsafe.Pointer(&rw.w))
		race.Disable()
	}
	// First, resolve competition with other writers.
	// 首先给互斥锁加锁
	rw.w.Lock()
	// Announce to readers there is a pending writer.
	// 这里将 readerCount 修改为负数
	r := rw.readerCount.Add(-rwmutexMaxReaders) + rwmutexMaxReaders
	// Wait for active readers.
	// 存在读协程并且修改 readerWait 
	if r != 0 && rw.readerWait.Add(r) != 0 {
		// 将读协程添加到 writerSem
		runtime_SemacquireRWMutex(&rw.writerSem, false, 0)
	}
	if race.Enabled {
		race.Enable()
		race.Acquire(unsafe.Pointer(&rw.readerSem))
		race.Acquire(unsafe.Pointer(&rw.writerSem))
	}
}

// Unlock unlocks rw for writing. It is a run-time error if rw is
// not locked for writing on entry to Unlock.
//
// As with Mutexes, a locked [RWMutex] is not associated with a particular
// goroutine. One goroutine may [RWMutex.RLock] ([RWMutex.Lock]) a RWMutex and then
// arrange for another goroutine to [RWMutex.RUnlock] ([RWMutex.Unlock]) it.
func (rw *RWMutex) Unlock() {
	if race.Enabled {
		race.Read(unsafe.Pointer(&rw.w))
		race.Release(unsafe.Pointer(&rw.readerSem))
		race.Disable()
	}

	// Announce to readers there is no active writer.
	// 加上 readerCount 修改为正值
	r := rw.readerCount.Add(rwmutexMaxReaders)
	// 
	if r >= rwmutexMaxReaders {
		race.Enable()
		fatal("sync: Unlock of unlocked RWMutex")
	}
	// Unblock blocked readers, if any.
	for i := 0; i < int(r); i++ {
		// 释放 readerSem 中的读协程
		runtime_Semrelease(&rw.readerSem, false, 0)
	}
	// Allow other writers to proceed.
	rw.w.Unlock()
	if race.Enabled {
		race.Enable()
	}
}

// RLock locks rw for reading.
//
// It should not be used for recursive read locking; a blocked Lock
// call excludes new readers from acquiring the lock. See the
// documentation on the [RWMutex] type.
func (rw *RWMutex) RLock() {
	if race.Enabled {
		race.Read(unsafe.Pointer(&rw.w))
		race.Disable()
	}
	if rw.readerCount.Add(1) < 0 {
		// A writer is pending, wait for it.
		runtime_SemacquireRWMutexR(&rw.readerSem, false, 0)
	}
	if race.Enabled {
		race.Enable()
		race.Acquire(unsafe.Pointer(&rw.readerSem))
	}
}

// RUnlock undoes a single [RWMutex.RLock] call;
// it does not affect other simultaneous readers.
// It is a run-time error if rw is not locked for reading
// on entry to RUnlock.
func (rw *RWMutex) RUnlock() {
	if race.Enabled {
		race.Read(unsafe.Pointer(&rw.w))
		race.ReleaseMerge(unsafe.Pointer(&rw.writerSem))
		race.Disable()
	}
	// 
	if r := rw.readerCount.Add(-1); r < 0 {
		// Outlined slow-path to allow the fast-path to be inlined
		rw.rUnlockSlow(r)
	}
	if race.Enabled {
		race.Enable()
	}
}

func (rw *RWMutex) rUnlockSlow(r int32) {
	if r+1 == 0 || r+1 == -rwmutexMaxReaders {
		race.Enable()
		fatal("sync: RUnlock of unlocked RWMutex")
	}
	// A writer is pending.
	if rw.readerWait.Add(-1) == 0 {
		// The last reader unblocks the writer.
		runtime_Semrelease(&rw.writerSem, false, 1)
	}
}

```

## 使用经验

![](image/Pasted%20image%2020250317145106.png)

## 总结

![](image/Pasted%20image%2020250317145153.png)

# WaitGroup

## 代码演示

```go
package main  
  
import (  
    "fmt"  
    "sync")  
  
type Person struct {  
    mu     sync.RWMutex  
    salary int  
    level  int  
}  
  
func promote(p *Person, wg *sync.WaitGroup) {  
    p.mu.Lock()  
    p.salary++  
    fmt.Println(p.salary)  
    p.level++  
    fmt.Println(p.level)  
    p.mu.Unlock()  
    wg.Done()  
}  
  
func printPerson(p *Person, wg *sync.WaitGroup) {  
    defer p.mu.RUnlock()  
    p.mu.RLock()  
    fmt.Println(p.salary)  
    fmt.Println(p.level)  
}  
  
func main() {  
  
    p := Person{level: 1, salary: 10000}  
    wg := sync.WaitGroup{}  
    wg.Add(3)  
    go promote(&p, &wg)  
    go promote(&p, &wg)  
    go promote(&p, &wg)  
    wg.Wait()  
  
}
```

## 需求

实际中等待接口调用

![](image/Pasted%20image%2020250317151726.png)

## sync.WaitGroup

![](image/Pasted%20image%2020250317152259.png)

### Wait 方法

![](image/Pasted%20image%2020250317152413.png)

### Done 方法

![](image/Pasted%20image%2020250317153551.png)

### Add 方法

![](image/Pasted%20image%2020250317153840.png)

## 源码

```go
// A WaitGroup waits for a collection of goroutines to finish.
// The main goroutine calls [WaitGroup.Add] to set the number of
// goroutines to wait for. Then each of the goroutines
// runs and calls [WaitGroup.Done] when finished. At the same time,
// [WaitGroup.Wait] can be used to block until all goroutines have finished.
//
// A WaitGroup must not be copied after first use.
//
// In the terminology of [the Go memory model], a call to [WaitGroup.Done]
// “synchronizes before” the return of any Wait call that it unblocks.
//
// [the Go memory model]: https://go.dev/ref/mem
type WaitGroup struct {
	noCopy noCopy

	state atomic.Uint64 // high 32 bits are counter, low 32 bits are waiter count.
	sema  uint32
}

// Wait blocks until the [WaitGroup] counter is zero.
func (wg *WaitGroup) Wait() {
	if race.Enabled {
		race.Disable()
	}
	for {
		state := wg.state.Load()
		// counter
		v := int32(state >> 32)
		// waiter
		w := uint32(state)
		// 计数器为 0 ，直接 return
		if v == 0 {
			// Counter is 0, no need to wait.
			if race.Enabled {
				race.Enable()
				race.Acquire(unsafe.Pointer(wg))
			}
			return
		}
		// Increment waiters count.
		// waiter + 1
		if wg.state.CompareAndSwap(state, state+1) {
			if race.Enabled && w == 0 {
				// Wait must be synchronized with the first Add.
				// Need to model this is as a write to race with the read in Add.
				// As a consequence, can do the write only for the first waiter,
				// otherwise concurrent Waits will race with each other.
				race.Write(unsafe.Pointer(&wg.sema))
			}
			// 陷入 sema，这样就添加了自己的信息，进入 sema
			runtime_SemacquireWaitGroup(&wg.sema)
			if wg.state.Load() != 0 {
				panic("sync: WaitGroup is reused before previous Wait has returned")
			}
			if race.Enabled {
				race.Enable()
				race.Acquire(unsafe.Pointer(wg))
			}
			return
		}
	}
}

// Done decrements the [WaitGroup] counter by one.
func (wg *WaitGroup) Done() {
	wg.Add(-1)
}

// Add adds delta, which may be negative, to the [WaitGroup] counter.
// If the counter becomes zero, all goroutines blocked on [WaitGroup.Wait] are released.
// If the counter goes negative, Add panics.
//
// Note that calls with a positive delta that occur when the counter is zero
// must happen before a Wait. Calls with a negative delta, or calls with a
// positive delta that start when the counter is greater than zero, may happen
// at any time.
// Typically this means the calls to Add should execute before the statement
// creating the goroutine or other event to be waited for.
// If a WaitGroup is reused to wait for several independent sets of events,
// new Add calls must happen after all previous Wait calls have returned.
// See the WaitGroup example.
func (wg *WaitGroup) Add(delta int) {
	if race.Enabled {
		if delta < 0 {
			// Synchronize decrements with Wait.
			race.ReleaseMerge(unsafe.Pointer(wg))
		}
		race.Disable()
		defer race.Enable()
	}
	// 核心逻辑
	state := wg.state.Add(uint64(delta) << 32)
	v := int32(state >> 32)
	w := uint32(state)
	if race.Enabled && delta > 0 && v == int32(delta) {
		// The first increment must be synchronized with Wait.
		// Need to model this as a read, because there can be
		// several concurrent wg.counter transitions from 0.
		race.Read(unsafe.Pointer(&wg.sema))
	}
	if v < 0 {
		panic("sync: negative WaitGroup counter")
	}
	if w != 0 && delta > 0 && v == int32(delta) {
		panic("sync: WaitGroup misuse: Add called concurrently with Wait")
	}
	if v > 0 || w == 0 {
		return
	}
	// This goroutine has set counter to 0 when waiters > 0.
	// Now there can't be concurrent mutations of state:
	// - Adds must not happen concurrently with Wait,
	// - Wait does not increment waiters if it sees counter == 0.
	// Still do a cheap sanity check to detect WaitGroup misuse.
	if wg.state.Load() != state {
		panic("sync: WaitGroup misuse: Add called concurrently with Wait")
	}
	// Reset waiters count to 0.
	wg.state.Store(0)
	for ; w != 0; w-- {
		runtime_Semrelease(&wg.sema, false, 0)
	}
}

```

## 总结

![](image/Pasted%20image%2020250317154000.png)

# 一段代码只能执行一次

## 代码演示

```go
package main

import (
	"fmt"
	"sync"
	"time"
)

type Person struct {
	mu     sync.RWMutex
	salary int
	level  int
}

func promote(p *Person) {
	p.mu.Lock()
	p.salary++
	fmt.Println(p.salary)
	p.level++
	fmt.Println(p.level)
	p.mu.Unlock()
}


func main() {
	p := Person{level: 1, salary: 10000}
	once := sync.Once{}

	go once.Do(func() {
		promote(&p)
	})
	go once.Do(func() {
		promote(&p)
	})
	go once.Do(func() {
		promote(&p)
	})
	time.Sleep(1 * time.Second)
}
```

## 需求

![](image/Pasted%20image%2020250317155200.png)

## 思路

![](image/Pasted%20image%2020250317155342.png)

### Atomic

![](image/Pasted%20image%2020250317155401.png)

### Mutex

![](image/Pasted%20image%2020250317155458.png)

## sync.Once

![](image/Pasted%20image%2020250317155702.png)

## 源码解析

```go
// Do calls the function f if and only if Do is being called for the
// first time for this instance of [Once]. In other words, given
//
//	var once Once
//
// if once.Do(f) is called multiple times, only the first call will invoke f,
// even if f has a different value in each invocation. A new instance of
// Once is required for each function to execute.
//
// Do is intended for initialization that must be run exactly once. Since f
// is niladic, it may be necessary to use a function literal to capture the
// arguments to a function to be invoked by Do:
//
//	config.once.Do(func() { config.init(filename) })
//
// Because no call to Do returns until the one call to f returns, if f causes
// Do to be called, it will deadlock.
//
// If f panics, Do considers it to have returned; future calls of Do return
// without calling f.
func (o *Once) Do(f func()) {
	// 注意：这里是一个错误的 Do 实现示例：
	//
	//	if o.done.CompareAndSwap(0, 1) {
	//		f()
	//	}
	//
	// Do 方法保证当它返回时，f 已经执行完成。
	// 但上述实现无法保证这一点：
	// 假设有两个并发调用，CAS 操作成功的调用者会执行 f，
	// 而第二个调用会立即返回，不会等待第一个调用的 f 执行完成。
	// 这就是为什么在慢路径（slow path）中需要回退使用互斥锁，
	// 且必须延迟 o.done.Store 操作到 f 返回之后的原因。

	if o.done.Load() == 0 {
		// Outlined slow-path to allow inlining of the fast-path.
		o.doSlow(f)
	}
}

func (o *Once) doSlow(f func()) {
	o.m.Lock()
	defer o.m.Unlock()
	if o.done.Load() == 0 {
		defer o.done.Store(1)
		f()
	}
}
```

## 总结

![](image/Pasted%20image%2020250317203709.png)

# 排查锁异常

## 锁拷贝问题

![](image/Pasted%20image%2020250318095902.png)

### 代码演示

永远不要拷贝现成的锁，很多情况都是疏忽带来的，比如一个通用的的结构体里面带锁，不小心复制了，但是 go 也提供工具检测锁拷贝

```go
package main  
  
import "sync"  
  
func main() {  
  
    mutex := sync.Mutex{}  
  
    mutex.Lock()  

  // 锁拷贝
    LockCopy := mutex  
  
    mutex.Unlock()  

  // 此时还是锁定
    LockCopy.Lock()  
  
}
```

### 执行结果

```
fatal error: all goroutines are asleep - deadlock!

goroutine 1 [sync.Mutex.Lock]:
internal/sync.runtime_SemacquireMutex(0xc000061f00?, 0x39?, 0xc0000021c0?)
	C:/Users/Han/sdk/go1.24.1/src/runtime/sema.go:95 +0x25
internal/sync.(*Mutex).lockSlow(0xc00000a070)
	C:/Users/Han/sdk/go1.24.1/src/internal/sync/mutex.go:149 +0x15d
internal/sync.(*Mutex).Lock(...)
	C:/Users/Han/sdk/go1.24.1/src/internal/sync/mutex.go:70
sync.(*Mutex).Lock(...)
	C:/Users/Han/sdk/go1.24.1/src/sync/mutex.go:45
main.main()
	C:/Users/Han/GolandProjects/GoRedis/main.go:15 +0x9e
```

### 如何检测

使用 `go vet main.go` 这样就可以检测锁拷贝

```shell
go vet main.go
# command-line-arguments
# [command-line-arguments]
.\main.go:11:14: assignment copies lock value to LockCopy: sync.Mutex
```

# RACE 竞争检测

![](image/Pasted%20image%2020250318134528.png)

```shell
go run -race main.go
==================
WARNING: DATA RACE
Read at 0x0001401bb168 by goroutine 7:
  main.do()
      C:/Users/Han/GolandProjects/GoRedis/main.go:6 +0x24

Previous write at 0x0001401bb168 by goroutine 6:
  main.do()
      C:/Users/Han/GolandProjects/GoRedis/main.go:6 +0x3c

Goroutine 7 (running) created at:
  main.main()
      C:/Users/Han/GolandProjects/GoRedis/main.go:12 +0x2c

Goroutine 6 (finished) created at:
  main.main()
      C:/Users/Han/GolandProjects/GoRedis/main.go:12 +0x2c
==================
==================
WARNING: DATA RACE
Write at 0x0001401bb168 by goroutine 9:
  main.do()
      C:/Users/Han/GolandProjects/GoRedis/main.go:6 +0x3c

Previous write at 0x0001401bb168 by goroutine 7:
  main.do()
      C:/Users/Han/GolandProjects/GoRedis/main.go:6 +0x3c

Goroutine 9 (running) created at:
  main.main()
      C:/Users/Han/GolandProjects/GoRedis/main.go:12 +0x2c

Goroutine 7 (finished) created at:
  main.main()
      C:/Users/Han/GolandProjects/GoRedis/main.go:12 +0x2c
==================
Found 2 data race(s)
exit status 66

```

## go-deadlock

![](image/Pasted%20image%2020250318134700.png)

## 总结

![](image/Pasted%20image%2020250318134716.png)

