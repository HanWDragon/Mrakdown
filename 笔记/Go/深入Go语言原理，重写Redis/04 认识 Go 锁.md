![](image/Pasted%20image%2020250311135407.png)

# 锁的基础是什么

是建立在两大基础之上，原子操作 + 信号锁

## atomic 操作

下面的代码，估计是 1000 ，但是实际结果并不是，这就是并发问题，原因是 `*p++` 相当于三步，读取，修改，写入

![](image/Pasted%20image%2020250311143442.png)

```go
package main  
  
import (  
    "fmt"  
    "time")  
  
func add(i *int32) {  
    *i++  
}  
  
func main() {  
    res := int32(0)  
    for i := 0; i < 1000; i++ {  
       go add(&res)  
    }  
    time.Sleep(time.Second)  
    fmt.Println(res)  
}
```

但是用如下的代码就没问题，去看看具体实现

![](image/Pasted%20image%2020250311145143.png)

```go
package main  
  
import (  
    "fmt"  
    "sync/atomic"    "time")  
  
func add(i *int32) {  
    //*i++  
    atomic.AddInt32(i, 1)  
}  
  
func main() {  
    res := int32(0)  
    for i := 0; i < 1000; i++ {  
       go add(&res)  
    }  
    time.Sleep(time.Second)  
    fmt.Println(res)  
}

// AddInt32 atomically adds delta to *addr and returns the new value.
// Consider using the more ergonomic and less error-prone [Int32.Add] instead.
//
//go:noescape
func AddInt32(addr *int32, delta int32) (new int32)

TEXT ·AddInt32(SB),NOSPLIT,$0  
    JMP    internal∕runtime∕atomic·Xadd(SB)

// uint32 Xadd(uint32 volatile *val, int32 delta)
// Atomically:
//	*val += delta;
//	return *val;
TEXT ·Xadd(SB), NOSPLIT, $0-20
	MOVQ	ptr+0(FP), BX
	MOVL	delta+8(FP), AX
	MOVL	AX, CX
	LOCK
	XADDL	AX, 0(BX)
	ADDL	CX, AX
	MOVL	AX, ret+16(FP)
	RET
```

## sema 锁

![](image/Pasted%20image%2020250311165021.png)

![](image/Pasted%20image%2020250311165500.png)

```go
// A semaRoot holds a balanced tree of sudog with distinct addresses (s.elem).
// Each of those sudog may in turn point (through s.waitlink) to a list
// of other sudogs waiting on the same address.
// The operations on the inner lists of sudogs with the same address
// are all O(1). The scanning of the top-level semaRoot list is O(log n),
// where n is the number of distinct addresses with goroutines blocked
// on them that hash to the given semaRoot.
// See golang.org/issue/17953 for a program that worked badly
// before we introduced the second level of list, and
// BenchmarkSemTable/OneAddrCollision/* for a benchmark that exercises this.
type semaRoot struct {
	lock  mutex
	treap *sudog        // root of balanced tree of unique waiters.
	nwait atomic.Uint32 // Number of waiters. Read w/o the lock.
}


// A Mutex is a mutual exclusion lock.
//
// See package [sync.Mutex] documentation.
type Mutex struct {
	state int32
	sema  uint32
}

// sudog (pseudo-g) represents a g in a wait list, such as for sending/receiving
// on a channel.
//
// sudog is necessary because the g ↔ synchronization object relation
// is many-to-many. A g can be on many wait lists, so there may be
// many sudogs for one g; and many gs may be waiting on the same
// synchronization object, so there may be many sudogs for one object.
//
// sudogs are allocated from a special pool. Use acquireSudog and
// releaseSudog to allocate and free them.
type sudog struct {
	// The following fields are protected by the hchan.lock of the
	// channel this sudog is blocking on. shrinkstack depends on
	// this for sudogs involved in channel ops.

	g *g

	next *sudog
	prev *sudog
	elem unsafe.Pointer // data element (may point to stack)

	// The following fields are never accessed concurrently.
	// For channels, waitlink is only accessed by g.
	// For semaphores, all fields (including the ones above)
	// are only accessed when holding a semaRoot lock.

	acquiretime int64
	releasetime int64
	ticket      uint32

	// isSelect indicates g is participating in a select, so
	// g.selectDone must be CAS'd to win the wake-up race.
	isSelect bool

	// success indicates whether communication over channel c
	// succeeded. It is true if the goroutine was awoken because a
	// value was delivered over channel c, and false if awoken
	// because c was closed.
	success bool

	// waiters is a count of semaRoot waiting list other than head of list,
	// clamped to a uint16 to fit in unused space.
	// Only meaningful at the head of the list.
	// (If we wanted to be overly clever, we could store a high 16 bits
	// in the second entry in the list.)
	waiters uint16

	parent   *sudog // semaRoot binary tree
	waitlink *sudog // g.waiting list or semaRoot
	waittail *sudog // semaRoot
	c        *hchan // channel
}

```

### sema 操作(uint32 > 0)

![](image/Pasted%20image%2020250311170423.png)

具体逻辑请参考[相关代码](#代码)

![](image/Pasted%20image%2020250311170531.png)

### sema 操作(uint 32 == 0)

![](image/Pasted%20image%2020250311171114.png)

### 代码

```go
// Called from runtime.  
func semacquire(addr *uint32) {  
    semacquire1(addr, false, 0, 0, waitReasonSemacquire)  
}  
  
func semacquire1(addr *uint32, lifo bool, profile semaProfileFlags, skipframes int, reason waitReason) {  
    gp := getg()  
    if gp != gp.m.curg {  
       throw("semacquire not on the G stack")  
    }  
  
    // Easy case.  
    if cansemacquire(addr) {  
       return  
    }  
  
    // Harder case:  
    // increment waiter count    // try cansemacquire one more time, return if succeeded    // enqueue itself as a waiter    // sleep    // (waiter descriptor is dequeued by signaler)    s := acquireSudog()  
    root := semtable.rootFor(addr)  
    t0 := int64(0)  
    s.releasetime = 0  
    s.acquiretime = 0  
    s.ticket = 0  
    if profile&semaBlockProfile != 0 && blockprofilerate > 0 {  
       t0 = cputicks()  
       s.releasetime = -1  
    }  
    if profile&semaMutexProfile != 0 && mutexprofilerate > 0 {  
       if t0 == 0 {  
          t0 = cputicks()  
       }  
       s.acquiretime = t0  
    }  
    for {  
       lockWithRank(&root.lock, lockRankRoot)  
       // Add ourselves to nwait to disable "easy case" in semrelease.  
       root.nwait.Add(1)  
       // Check cansemacquire to avoid missed wakeup.  
       if cansemacquire(addr) {  
          root.nwait.Add(-1)  
          unlock(&root.lock)  
          break  
       }  
       // Any semrelease after the cansemacquire knows we're waiting  
       // (we set nwait above), so go to sleep.       root.queue(addr, s, lifo)  
       goparkunlock(&root.lock, reason, traceBlockSync, 4+skipframes)  
       if s.ticket != 0 || cansemacquire(addr) {  
          break  
       }  
    }  
    if s.releasetime > 0 {  
       blockevent(s.releasetime-t0, 3+skipframes)  
    }  
    releaseSudog(s)  
}


func cansemacquire(addr *uint32) bool {
	for {
		v := atomic.Load(addr)
		if v == 0 {
			return false
		}
		if atomic.Cas(addr, v, v-1) {
			return true
		}
	}
}

func semrelease(addr *uint32) {
	semrelease1(addr, false, 0)
}

func semrelease1(addr *uint32, handoff bool, skipframes int) {
	root := semtable.rootFor(addr)
	atomic.Xadd(addr, 1)

	// Easy case: no waiters?
	// This check must happen after the xadd, to avoid a missed wakeup
	// (see loop in semacquire).
	if root.nwait.Load() == 0 {
		return
	}

	// Harder case: search for a waiter and wake it.
	lockWithRank(&root.lock, lockRankRoot)
	if root.nwait.Load() == 0 {
		// The count is already consumed by another goroutine,
		// so no need to wake up another goroutine.
		unlock(&root.lock)
		return
	}
	s, t0, tailtime := root.dequeue(addr)
	if s != nil {
		root.nwait.Add(-1)
	}
	unlock(&root.lock)
	if s != nil { // May be slow or even yield, so unlock first
		acquiretime := s.acquiretime
		if acquiretime != 0 {
			// Charge contention that this (delayed) unlock caused.
			// If there are N more goroutines waiting beyond the
			// one that's waking up, charge their delay as well, so that
			// contention holding up many goroutines shows up as
			// more costly than contention holding up a single goroutine.
			// It would take O(N) time to calculate how long each goroutine
			// has been waiting, so instead we charge avg(head-wait, tail-wait)*N.
			// head-wait is the longest wait and tail-wait is the shortest.
			// (When we do a lifo insertion, we preserve this property by
			// copying the old head's acquiretime into the inserted new head.
			// In that case the overall average may be slightly high, but that's fine:
			// the average of the ends is only an approximation to the actual
			// average anyway.)
			// The root.dequeue above changed the head and tail acquiretime
			// to the current time, so the next unlock will not re-count this contention.
			dt0 := t0 - acquiretime
			dt := dt0
			if s.waiters != 0 {
				dtail := t0 - tailtime
				dt += (dtail + dt0) / 2 * int64(s.waiters)
			}
			mutexevent(dt, 3+skipframes)
		}
		if s.ticket != 0 {
			throw("corrupted semaphore ticket")
		}
		if handoff && cansemacquire(addr) {
			s.ticket = 1
		}
		readyWithTime(s, 5+skipframes)
		if s.ticket == 1 && getg().m.locks == 0 {
			// Direct G handoff
			// readyWithTime has added the waiter G as runnext in the
			// current P; we now call the scheduler so that we start running
			// the waiter G immediately.
			// Note that waiter inherits our time slice: this is desirable
			// to avoid having a highly contended semaphore hog the P
			// indefinitely. goyield is like Gosched, but it emits a
			// "preempted" trace event instead and, more importantly, puts
			// the current G on the local runq instead of the global one.
			// We only do this in the starving regime (handoff=true), as in
			// the non-starving case it is possible for a different waiter
			// to acquire the semaphore while we are yielding/scheduling,
			// and this would be wasteful. We wait instead to enter starving
			// regime, and then we start to do direct handoffs of ticket and
			// P.
			// See issue 33747 for discussion.
			goyield()
		}
	}
}

```

## 总结

![](image/Pasted%20image%2020250311171229.png)

# 互斥锁

![](image/Pasted%20image%2020250311171656.png)

在结构体中添加锁

```go
package main

import (
	"fmt"
	"sync"
	"time"
)

type Person struct {
	mu     sync.Mutex
	salary int
	level  int
}

func promote(p *Person) {
	p.mu.Lock()
	p.salary++
	fmt.Println(p.salary)
	p.level++
	fmt.Println(p.level)
	p.mu.Unlock()
}

func main() {

	p := Person{level: 1, salary: 10000}

	go promote(&p)
	go promote(&p)
	go promote(&p)

	time.Sleep(time.Second)

}
```

还有一种方式是，但是这种方案就像自旋锁，线程会空转，没有触发基于信号的调度就会一直运行，竞争激烈不可取

```go
package main

import (
	"fmt"
	"sync/atomic"
	"time"
)

type Person struct {
	mu     atomic.Int32
	salary int
	level  int
}

func promote(p *Person) {
	p.mu.CompareAndSwap(0, 1)
	p.salary++
	fmt.Println(p.salary)
	p.level++
	fmt.Println(p.level)
	p.mu.CompareAndSwap(1, 0)
}

func main() {

	p := Person{level: 1, salary: 10000}

	go promote(&p)
	go promote(&p)
	go promote(&p)

	time.Sleep(time.Second)

}
```

## sync.mutex

state 低三位都代表不同含义，剩下高 29 位表示等待的数量，由于 sema 初始化为 0 ，可以看作阻塞队列，state 表示多个含义，低 3 位 表示饥饿、醒来、锁住，高 29 位是排队数量

![](image/Pasted%20image%2020250311205201.png)

### 正常模式加锁

![](image/Pasted%20image%2020250311205527.png)

解释下下面两张图，褐色就是获取到锁，带箭头就是自旋尝试，一定次数没抢到就到 SeamRoot 去休眠，相当于阻塞队列，有协程忙完了就唤醒阻塞的协程

![](image/Pasted%20image%2020250311205934.png)

![](image/Pasted%20image%2020250311205947.png)

```go

```